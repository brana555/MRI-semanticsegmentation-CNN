{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2guthgOz-Ssm"
   },
   "source": [
    "# Coursework 2 for Cardiac MR Image Segmentation (2020-2021)\n",
    "\n",
    "After you have gone through the coursework description, this tutorial is designed to further helps you understand the problem and therefore enable you to propose a good solution for this coursework. You will learn:\n",
    "\n",
    "* how to load and save images with OpenCV\n",
    "* how to train a segmentation model with Pytorch\n",
    "* how to evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5cMT935YxzG",
    "outputId": "bd9c2281-9891-4525-9c76-71e4ad96232e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import torch\n",
    "\n",
    "#if torch.cuda.is_available():       \n",
    "#    device = torch.device(\"cuda\")\n",
    "#    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "#    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "#else:\n",
    " #   print('No GPU available, using the CPU instead.')\n",
    "  #  device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvIIlzhRc94u",
    "outputId": "37be2e52-0d8c-4486-e23c-320abe5cf7e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wtRc3GKuCuR",
    "outputId": "450dcd1f-adb2-4165-c008-18a994581b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
      "\n",
      "Usage:   \n",
      "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip3 install [options] [-e] <vcs project url> ...\n",
      "  pip3 install [options] [-e] <local project path> ...\n",
      "  pip3 install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -u\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch\n",
    "#!pip install -u numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsnVbP35-Sso"
   },
   "source": [
    "## 1. Load, show, and save images with OpenCV\n",
    "\n",
    "> Indented block\n",
    "\n",
    "\n",
    "\n",
    "OpenCV is an open-source computer vision library which helps us to manipulate image data. In this section, we will cover:\n",
    "* Loading an image from file with imread()\n",
    "* Displaying the image with matplotlib plt.imshow()\n",
    "* Saving an image with imwrite()\n",
    "\n",
    "For a more comprehensive study of OpenCV, we encourage you to check the official [OpenCV documentation](https://docs.opencv.org/master/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2Sbd1SFtnNp",
    "outputId": "465bd4a3-9784-4f61-c5d4-a413d83aacb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open CW2.zip, CW2.zip.zip or CW2.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "#!unzip CW2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C7ZvSiY3qW_U"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def show_image_mask(img, mask, cmap='gray'): # visualisation\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap=cmap)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "EN5WJ_XG-Sso",
    "outputId": "8e2e0003-10f1-4b43-dd76-3fa36c597077"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2 #import OpenCV\n",
    "\n",
    "data_dir = './data/train' #'./data/train'\n",
    "image = cv2.imread(os.path.join(data_dir,'image','cmr100.png'), cv2.IMREAD_UNCHANGED)\n",
    "mask = cv2.imread(os.path.join(data_dir,'mask','cmr100_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "show_image_mask(image, mask, cmap='gray')\n",
    "plt.pause(1)\n",
    "cv2.imwrite(os.path.join('./','cmr100.png'), mask*1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UDvsGZnYfHS"
   },
   "source": [
    "Note: You will no doubt notice that the mask images appear to be completely black with no sign of any segmentations. This is because the max intensity of pixels in an 8-bit png image is 255 and your image viewer software only sees 255 as white. For those values close to zero, you will only see dark values. This is the case for our masks as the background, the right ventricle, the myocardium, and the left ventricle in each image are 0, 1, 2, and 3, respectively. All of which are close to zero. If we multiply the original mask by 85 and save the result to the directory where this code is, we can see the heart indeed shows up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hULAX3WH-Sss"
   },
   "source": [
    "## 2 Define a segmentation model with Pytorch\n",
    "\n",
    "In this section, we expect you to learn how to:\n",
    "* Define a Segmentation Model\n",
    "* Define a DataLoader that inputs images to the Model\n",
    "* Define training parameters and train the model\n",
    "* Test the trained model with a new input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrKFgoZvUbeg"
   },
   "source": [
    "### 2.1 Define a DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC9s43MqqW_U"
   },
   "source": [
    "Below we provide you with a dataloader to use in your assigment. You will only need to focus on the development of your model and loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XYrD95T8qz8T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "        self.mask_files = []\n",
    "        for img_path in self.img_files:\n",
    "            basename = os.path.basename(img_path)\n",
    "            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float(), torch.from_numpy(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82UAfnwSUgc_"
   },
   "source": [
    "### 2.2 Define a Segmenatation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEIkCqdfYnIn"
   },
   "source": [
    "You will need to define your CNN model for segmentation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-W6532hFXa_g"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import Conv2d, Dropout2d, MaxPool2d, ReLU, UpsamplingNearest2d\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "vgg16_dims = [\n",
    "                    (64, 64, 'M'),                                # Stage - 1\n",
    "                    (128, 128, 'M'),                              # Stage - 2\n",
    "                    (256, 256, 256,'M'),                          # Stage - 3\n",
    "                    (512, 512, 512, 'M'),                         # Stage - 4\n",
    "                    (512, 512, 512, 'M')                          # Stage - 5\n",
    "            ]\n",
    "decoder_dims = [\n",
    "                    ('U', 512, 512, 512),                         # Stage - 5\n",
    "                    ('U', 512, 512, 512),                         # Stage - 4\n",
    "                    ('U', 256, 256, 256),                         # Stage - 3\n",
    "                    ('U', 128, 128),                              # Stage - 2\n",
    "                    ('U', 64, 64)                                 # Stage - 1\n",
    "                ]\n",
    "\n",
    "class SegNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegNet, self).__init__()\n",
    "\n",
    "        self.input_channels = 1\n",
    "        self.output_channels = 4\n",
    "        self.num_channels = 1\n",
    "        self.vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_conv_00 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=self.input_channels, \n",
    "                                                          out_channels=64, \n",
    "                                                          kernel_size=3, \n",
    "                                                          padding=1), \n",
    "                                               nn.BatchNorm2d(64)])                                         \n",
    "        self.encoder_conv_01 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=64,\n",
    "                                                          out_channels=64,\n",
    "                                                          kernel_size=3,\n",
    "                                                          padding=1),\n",
    "                                                nn.BatchNorm2d(64)\n",
    "                                                ])\n",
    "        self.encoder_conv_10 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=64,\n",
    "                                                          out_channels=128,\n",
    "                                                          kernel_size=3,\n",
    "                                                          padding=1),\n",
    "                                                nn.BatchNorm2d(128)\n",
    "                                                ])\n",
    "        self.encoder_conv_11 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=128,\n",
    "                                                          out_channels=128,\n",
    "                                                          kernel_size=3,\n",
    "                                                          padding=1),\n",
    "                                                nn.BatchNorm2d(128)\n",
    "                                                ])\n",
    "        self.encoder_conv_20 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=128,\n",
    "                                                          out_channels=256,\n",
    "                                                          kernel_size=3,\n",
    "                                                          padding=1),\n",
    "                                                nn.BatchNorm2d(256)\n",
    "                                                ])\n",
    "        self.encoder_conv_21 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=256,\n",
    "                                                          out_channels=256,\n",
    "                                                          kernel_size=3,\n",
    "                                                          padding=1),\n",
    "                                                nn.BatchNorm2d(256)\n",
    "                                                ])\n",
    "        self.encoder_conv_22 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=256,\n",
    "                                                          out_channels=256,\n",
    "                                                          kernel_size=3,\n",
    "                                                          padding=1),\n",
    "                                                nn.BatchNorm2d(256)\n",
    "                                                ])\n",
    "        self.encoder_conv_30 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=256,\n",
    "                                                          out_channels=512,\n",
    "                                                          kernel_size=3,\n",
    "                                                          padding=1),\n",
    "                                                nn.BatchNorm2d(512)\n",
    "                                                ])\n",
    "        self.encoder_conv_31 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=512,\n",
    "                                                          out_channels=512,\n",
    "                                                          kernel_size=3,\n",
    "                                                          padding=1),\n",
    "                                                nn.BatchNorm2d(512)\n",
    "                                                ])\n",
    "        self.encoder_conv_32 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=512,\n",
    "                                                          out_channels=512,\n",
    "                                                          kernel_size=3,\n",
    "                                                          padding=1),\n",
    "                                                nn.BatchNorm2d(512)\n",
    "                                                ])\n",
    "        self.encoder_conv_40 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=512,\n",
    "                                                          out_channels=512,\n",
    "                                                          kernel_size=3,\n",
    "                                                          padding=1),\n",
    "                                                nn.BatchNorm2d(512)\n",
    "                                                ])\n",
    "        self.encoder_conv_41 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=512,\n",
    "                                                          out_channels=512,\n",
    "                                                          kernel_size=3,\n",
    "                                                          padding=1),\n",
    "                                                nn.BatchNorm2d(512)\n",
    "                                                ])\n",
    "        self.encoder_conv_42 = nn.Sequential(*[\n",
    "                                                nn.Conv2d(in_channels=512,\n",
    "                                                          out_channels=512,\n",
    "                                                          kernel_size=3,\n",
    "                                                          padding=1),\n",
    "                                                nn.BatchNorm2d(512)\n",
    "                                                ])\n",
    "        self.init_vgg_weigts()\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_convtr_42 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=512,\n",
    "                                                                   out_channels=512,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1),\n",
    "                                                nn.BatchNorm2d(512)\n",
    "                                               ])\n",
    "        self.decoder_convtr_41 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=512,\n",
    "                                                                   out_channels=512,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1),\n",
    "                                                nn.BatchNorm2d(512)\n",
    "                                               ])\n",
    "        self.decoder_convtr_40 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=512,\n",
    "                                                                   out_channels=512,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1),\n",
    "                                                nn.BatchNorm2d(512)\n",
    "                                               ])\n",
    "        self.decoder_convtr_32 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=512,\n",
    "                                                                   out_channels=512,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1),\n",
    "                                                nn.BatchNorm2d(512)\n",
    "                                               ])\n",
    "        self.decoder_convtr_31 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=512,\n",
    "                                                                   out_channels=512,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1),\n",
    "                                                nn.BatchNorm2d(512)\n",
    "                                               ])\n",
    "        self.decoder_convtr_30 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=512,\n",
    "                                                                   out_channels=256,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1),\n",
    "                                                nn.BatchNorm2d(256)\n",
    "                                               ])\n",
    "        self.decoder_convtr_22 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=256,\n",
    "                                                                   out_channels=256,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1),\n",
    "                                                nn.BatchNorm2d(256)\n",
    "                                               ])\n",
    "        self.decoder_convtr_21 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=256,\n",
    "                                                                   out_channels=256,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1),\n",
    "                                                nn.BatchNorm2d(256)\n",
    "                                               ])\n",
    "        self.decoder_convtr_20 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=256,\n",
    "                                                                   out_channels=128,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1),\n",
    "                                                nn.BatchNorm2d(128)\n",
    "                                               ])\n",
    "        self.decoder_convtr_11 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=128,\n",
    "                                                                   out_channels=128,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1),\n",
    "                                                nn.BatchNorm2d(128)\n",
    "                                               ])\n",
    "        self.decoder_convtr_10 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=128,\n",
    "                                                                   out_channels=64,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1),\n",
    "                                                nn.BatchNorm2d(64)\n",
    "                                               ])\n",
    "        self.decoder_convtr_01 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=64,\n",
    "                                                                   out_channels=64,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1),\n",
    "                                                nn.BatchNorm2d(64)\n",
    "                                               ])\n",
    "        self.decoder_convtr_00 = nn.Sequential(*[\n",
    "                                                nn.ConvTranspose2d(in_channels=64,\n",
    "                                                                   out_channels=self.output_channels,\n",
    "                                                                   kernel_size=3,\n",
    "                                                                   padding=1)\n",
    "                                               ])\n",
    "\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        \"\"\"\n",
    "        Forward pass `input_img` through the network\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        # Encoder Stage - 1\n",
    "        dim_0 = input_img.size()\n",
    "        x_00 = F.relu(self.encoder_conv_00(input_img))\n",
    "        x_01 = F.relu(self.encoder_conv_01(x_00))\n",
    "        x_0, indices_0 = F.max_pool2d(x_01, kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "        # Encoder Stage - 2\n",
    "        dim_1 = x_0.size()\n",
    "        x_10 = F.relu(self.encoder_conv_10(x_0))\n",
    "        x_11 = F.relu(self.encoder_conv_11(x_10))\n",
    "        x_1, indices_1 = F.max_pool2d(x_11, kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "        # Encoder Stage - 3\n",
    "        dim_2 = x_1.size()\n",
    "        x_20 = F.relu(self.encoder_conv_20(x_1))\n",
    "        x_21 = F.relu(self.encoder_conv_21(x_20))\n",
    "        x_22 = F.relu(self.encoder_conv_22(x_21))\n",
    "        x_2, indices_2 = F.max_pool2d(x_22, kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "        # Encoder Stage - 4\n",
    "        dim_3 = x_2.size()\n",
    "        x_30 = F.relu(self.encoder_conv_30(x_2))\n",
    "        x_31 = F.relu(self.encoder_conv_31(x_30))\n",
    "        x_32 = F.relu(self.encoder_conv_32(x_31))\n",
    "        x_3, indices_3 = F.max_pool2d(x_32, kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "        # Encoder Stage - 5\n",
    "        dim_4 = x_3.size()\n",
    "        x_40 = F.relu(self.encoder_conv_40(x_3))\n",
    "        x_41 = F.relu(self.encoder_conv_41(x_40))\n",
    "        x_42 = F.relu(self.encoder_conv_42(x_41))\n",
    "        x_4, indices_4 = F.max_pool2d(x_42, kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "        # Decoder\n",
    "\n",
    "        dim_d = x_4.size()\n",
    "\n",
    "        # Decoder Stage - 5\n",
    "        x_4d = F.max_unpool2d(x_4, indices_4, kernel_size=2, stride=2, output_size=dim_4)\n",
    "        x_42d = F.relu(self.decoder_convtr_42(x_4d))\n",
    "        x_41d = F.relu(self.decoder_convtr_41(x_42d))\n",
    "        x_40d = F.relu(self.decoder_convtr_40(x_41d))\n",
    "        dim_4d = x_40d.size()\n",
    "\n",
    "        # Decoder Stage - 4\n",
    "        x_3d = F.max_unpool2d(x_40d, indices_3, kernel_size=2, stride=2, output_size=dim_3)\n",
    "        x_32d = F.relu(self.decoder_convtr_32(x_3d))\n",
    "        x_31d = F.relu(self.decoder_convtr_31(x_32d))\n",
    "        x_30d = F.relu(self.decoder_convtr_30(x_31d))\n",
    "        dim_3d = x_30d.size()\n",
    "\n",
    "        # Decoder Stage - 3\n",
    "        x_2d = F.max_unpool2d(x_30d, indices_2, kernel_size=2, stride=2, output_size=dim_2)\n",
    "        x_22d = F.relu(self.decoder_convtr_22(x_2d))\n",
    "        x_21d = F.relu(self.decoder_convtr_21(x_22d))\n",
    "        x_20d = F.relu(self.decoder_convtr_20(x_21d))\n",
    "        dim_2d = x_20d.size()\n",
    "\n",
    "        # Decoder Stage - 2\n",
    "        x_1d = F.max_unpool2d(x_20d, indices_1, kernel_size=2, stride=2, output_size=dim_1)\n",
    "        x_11d = F.relu(self.decoder_convtr_11(x_1d))\n",
    "        x_10d = F.relu(self.decoder_convtr_10(x_11d))\n",
    "        dim_1d = x_10d.size()\n",
    "\n",
    "        # Decoder Stage - 1\n",
    "        x_0d = F.max_unpool2d(x_10d, indices_0, kernel_size=2, stride=2, output_size=dim_0)\n",
    "        x_01d = F.relu(self.decoder_convtr_01(x_0d))\n",
    "        x_00d = self.decoder_convtr_00(x_01d)\n",
    "        dim_0d = x_00d.size()\n",
    "        return x_00d\n",
    "        #x_softmax = F.softmax(x_00d, dim=1)\n",
    "        #return x_00d, x_softmax\n",
    "\n",
    "    def init_vgg_weigts(self):\n",
    "        #assert self.encoder_conv_00[0].weight.size() == self.vgg16.features[0].weight.size()\n",
    "        self.encoder_conv_00[0].weight.data = self.vgg16.features[0].weight.data\n",
    "        #assert self.encoder_conv_00[0].bias.size() == self.vgg16.features[0].bias.size()\n",
    "        self.encoder_conv_00[0].bias.data = self.vgg16.features[0].bias.data\n",
    "\n",
    "        #assert self.encoder_conv_01[0].weight.size() == self.vgg16.features[2].weight.size()\n",
    "        self.encoder_conv_01[0].weight.data = self.vgg16.features[2].weight.data\n",
    "        #assert self.encoder_conv_01[0].bias.size() == self.vgg16.features[2].bias.size()\n",
    "        self.encoder_conv_01[0].bias.data = self.vgg16.features[2].bias.data\n",
    "\n",
    "        #assert self.encoder_conv_10[0].weight.size() == self.vgg16.features[5].weight.size()\n",
    "        self.encoder_conv_10[0].weight.data = self.vgg16.features[5].weight.data\n",
    "        #assert self.encoder_conv_10[0].bias.size() == self.vgg16.features[5].bias.size()\n",
    "        self.encoder_conv_10[0].bias.data = self.vgg16.features[5].bias.data\n",
    "\n",
    "        #assert self.encoder_conv_11[0].weight.size() == self.vgg16.features[7].weight.size()\n",
    "        self.encoder_conv_11[0].weight.data = self.vgg16.features[7].weight.data\n",
    "        #assert self.encoder_conv_11[0].bias.size() == self.vgg16.features[7].bias.size()\n",
    "        self.encoder_conv_11[0].bias.data = self.vgg16.features[7].bias.data\n",
    "\n",
    "        #assert self.encoder_conv_20[0].weight.size() == self.vgg16.features[10].weight.size()\n",
    "        self.encoder_conv_20[0].weight.data = self.vgg16.features[10].weight.data\n",
    "        #assert self.encoder_conv_20[0].bias.size() == self.vgg16.features[10].bias.size()\n",
    "        self.encoder_conv_20[0].bias.data = self.vgg16.features[10].bias.data\n",
    "\n",
    "        #assert self.encoder_conv_21[0].weight.size() == self.vgg16.features[12].weight.size()\n",
    "        self.encoder_conv_21[0].weight.data = self.vgg16.features[12].weight.data\n",
    "        #assert self.encoder_conv_21[0].bias.size() == self.vgg16.features[12].bias.size()\n",
    "        self.encoder_conv_21[0].bias.data = self.vgg16.features[12].bias.data\n",
    "\n",
    "        #assert self.encoder_conv_22[0].weight.size() == self.vgg16.features[14].weight.size()\n",
    "        self.encoder_conv_22[0].weight.data = self.vgg16.features[14].weight.data\n",
    "        #assert self.encoder_conv_22[0].bias.size() == self.vgg16.features[14].bias.size()\n",
    "        self.encoder_conv_22[0].bias.data = self.vgg16.features[14].bias.data\n",
    "\n",
    "        #assert self.encoder_conv_30[0].weight.size() == self.vgg16.features[17].weight.size()\n",
    "        self.encoder_conv_30[0].weight.data = self.vgg16.features[17].weight.data\n",
    "        #assert self.encoder_conv_30[0].bias.size() == self.vgg16.features[17].bias.size()\n",
    "        self.encoder_conv_30[0].bias.data = self.vgg16.features[17].bias.data\n",
    "\n",
    "        #assert self.encoder_conv_31[0].weight.size() == self.vgg16.features[19].weight.size()\n",
    "        self.encoder_conv_31[0].weight.data = self.vgg16.features[19].weight.data\n",
    "        #assert self.encoder_conv_31[0].bias.size() == self.vgg16.features[19].bias.size()\n",
    "        self.encoder_conv_31[0].bias.data = self.vgg16.features[19].bias.data\n",
    "\n",
    "        #assert self.encoder_conv_32[0].weight.size() == self.vgg16.features[21].weight.size()\n",
    "        self.encoder_conv_32[0].weight.data = self.vgg16.features[21].weight.data\n",
    "        #assert self.encoder_conv_32[0].bias.size() == self.vgg16.features[21].bias.size()\n",
    "        self.encoder_conv_32[0].bias.data = self.vgg16.features[21].bias.data\n",
    "\n",
    "        #assert self.encoder_conv_40[0].weight.size() == self.vgg16.features[24].weight.size()\n",
    "        self.encoder_conv_40[0].weight.data = self.vgg16.features[24].weight.data\n",
    "        #assert self.encoder_conv_40[0].bias.size() == self.vgg16.features[24].bias.size()\n",
    "        self.encoder_conv_40[0].bias.data = self.vgg16.features[24].bias.data\n",
    "\n",
    "        #assert self.encoder_conv_41[0].weight.size() == self.vgg16.features[26].weight.size()\n",
    "        self.encoder_conv_41[0].weight.data = self.vgg16.features[26].weight.data\n",
    "        #assert self.encoder_conv_41[0].bias.size() == self.vgg16.features[26].bias.size()\n",
    "        self.encoder_conv_41[0].bias.data = self.vgg16.features[26].bias.data\n",
    "\n",
    "        #assert self.encoder_conv_42[0].weight.size() == self.vgg16.features[28].weight.size()\n",
    "        self.encoder_conv_42[0].weight.data = self.vgg16.features[28].weight.data\n",
    "        #assert self.encoder_conv_42[0].bias.size() == self.vgg16.features[28].bias.size()\n",
    "        self.encoder_conv_42[0].bias.data = self.vgg16.features[28].bias.data\n",
    "\n",
    "model = SegNet() # We can now create a model using your defined segmentation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRdPFTa9a34J"
   },
   "source": [
    "### 2.3 Define a Loss function and optimizer\n",
    "\n",
    "You will need to define a loss function and an optimizer. torch.nn has a variety of readymade loss functions, although you may wish to create your own instead. torch.optim has a variety of optimizers, it is advised that you use one of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QRjOZGXRbUFT"
   },
   "outputs": [],
   "source": [
    "Loss = nn.CrossEntropyLoss() \n",
    "#Loss=  torch.nn.HingeEmbeddingLoss()    - Hinge Embedding\n",
    "#Loss= torch.nn.KLDivLoss() - Kullback-Leibler divergence\n",
    "#Loss = torch.nn.NLLLoss()\n",
    "#Loss = torch.nn.MSELoss()\n",
    "#optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "#optimiser = torch.optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "#optimiser=torch.optim.SparseAdam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "#optimiser=torch.optim.SGD(model.parameters(), lr=0.1, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "#optimiser= torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grDz3fR1qW_V"
   },
   "source": [
    "### 2.4 Training\n",
    "\n",
    "As most of you will use CPUs to train the model, expect your models to take **30 minutes to train if not longer depending on network architecture**. To save time, you should not be using all training data until your model is well developed. If you are running your model on a GPU training should be significantly faster. During the training process, you may want to save the checkpoints as follows:\n",
    "\n",
    "```\n",
    "# Saving checkpoints for validation/testing\n",
    "torch.save(model.state_dict(), path)\n",
    "```\n",
    "The saved checkpoints can be used to load at a later date for validation and testing. Here we give some example code for training a model. Note that you need to specify the max iterations you want to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCb4bxVVchxf",
    "outputId": "ebdcb77b-6b4a-41ff-fac1-f1035e46d5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "[1] training loss: 0.944\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "validation loss: 1.041850\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "[2] training loss: 0.406\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "validation loss: 0.479994\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "[3] training loss: 0.267\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "validation loss: 0.304033\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "[4] training loss: 0.174\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "validation loss: 0.471392\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "[5] training loss: 0.130\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "validation loss: 0.221087\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n",
      "output size: torch.Size([10, 4, 96, 96])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4ef036e8121e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Did this to get input in RGB format, I am going to try and remove this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#print(img.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1f5daaa09b6d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_img)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx_3d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_unpool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_40d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mx_32d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_convtr_32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_3d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mx_31d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_convtr_31\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_32d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mx_30d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_convtr_30\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_31d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mdim_3d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_30d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0moutput_padding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m             output_padding, self.groups, self.dilation)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "def closure():\n",
    "  optimizer.zero_grad()\n",
    "  # Forward pass\n",
    "        #outputs = model(torch.reshape(img[None, ...],(batch_size,1,96,96)))\n",
    "  outputs = model(img.unsqueeze(1))\n",
    "  loss = Loss(outputs, mask.long())\n",
    "  # Then write your BACKWARD & OPTIMIZE below\n",
    "        # Note: Compute Loss and Optimize\n",
    "        # Backward and optimize\n",
    "  loss.backward()\n",
    "  return loss\n",
    "    \n",
    "data_path_train = './data/train'\n",
    "num_workers = 4\n",
    "batch_size = 10 #5\n",
    "train_set = TrainDataset(data_path_train)\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
    "data_path_val = './data/val'\n",
    "eval_set = TrainDataset(data_path_val)\n",
    "eval_data_loader = DataLoader(dataset=eval_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
    "epochs = 10 #10\n",
    "running_loss_eval_log=[]\n",
    "for epoch in range(epochs):\n",
    "    running_loss_train=0\n",
    "    running_loss_eval=0\n",
    "    # Fetch images and labels.\n",
    "    model.train()\n",
    "    for iteration, sample in enumerate(training_data_loader):\n",
    "\n",
    "        img, mask = sample\n",
    "        #show_image_mask(img[0,...].squeeze(), mask[0,...].squeeze()) #visualise all data in training set\n",
    "        plt.pause(1)\n",
    "        optimizer.zero_grad()\n",
    "        # Write your FORWARD below\n",
    "        # Note: Input image to your model and ouput the predicted mask and Your predicted mask should have 4 channels\n",
    "        # Forward pass\n",
    "        #outputs = model(torch.reshape(img[None, ...],(batch_size,1,96,96)))\n",
    "\n",
    "\n",
    "        img = img.unsqueeze(1).repeat(1, 3, 1, 1) #Did this to get input in RGB format, I am going to try and remove this\n",
    "        #print(img.size())\n",
    "        outputs = model(img)\n",
    "        print(\"output size:\", outputs.size())\n",
    "\n",
    "\n",
    "        #print(mask.long().size())\n",
    "        loss = Loss(outputs, mask.long())\n",
    "        # Then write your BACKWARD & OPTIMIZE below\n",
    "        # Note: Compute Loss and Optimize\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss_train += loss.item()\n",
    "        if iteration==(100/batch_size-1):    # print every 2000 mini-batches\n",
    "            print('[%d] training loss: %.3f' %\n",
    "                  (epoch + 1, running_loss_train / (100/batch_size)))\n",
    "            running_loss_train = 0.0\n",
    "        \n",
    "        #a = torch.argmax(outputs.squeeze(), dim=0).detach().cpu().numpy()\n",
    "        #print(np.unique(a))\n",
    "        #show_image_mask(a, mask[0,...].squeeze())\n",
    "        \n",
    "        plt.pause(1)\n",
    "\n",
    "    #evaluate the data\n",
    "    model.eval()\n",
    "    # Fetch images and labels.  \n",
    "    for iteration, sample in enumerate(eval_data_loader):\n",
    "        with torch.no_grad():\n",
    "            img, mask = sample\n",
    "            #show_image_mask(img[0,...].squeeze(), mask[0,...].squeeze()) #visualise all data in training set\n",
    "            plt.pause(1)\n",
    "            # Write your FORWARD below\n",
    "            # Note: Input image to your model and ouput the predicted mask and Your predicted mask should have 4 channels\n",
    "            # Forward pass\n",
    "\n",
    "\n",
    "            img = img.unsqueeze(1).repeat(1, 3, 1, 1) #Did this to get input in RGB format, I am going to try and remove this\n",
    "            outputs = model(img)\n",
    "            loss = Loss(outputs, mask.long())\n",
    "            print(\"output size:\", outputs.size())\n",
    "\n",
    "\n",
    "            #print(mask.size())\n",
    "            running_loss_eval += loss.item()\n",
    "            if iteration==(20/batch_size-1):    # print every 2000 mini-batches\n",
    "                print('validation loss: %.6f' %\n",
    "                  (running_loss_eval / (20/batch_size)))\n",
    "                #np.append(running_loss_eval_log,running_loss_eval / (20/batch_size))\n",
    "                running_loss_eval_log.append(running_loss_eval / (20/batch_size))\n",
    "                running_loss_eval = 0.0\n",
    "    #early stopping - think this is faulty\n",
    "    #if epoch>=3:\n",
    "    #   if (running_loss_eval_log[epoch]>running_loss_eval_log[epoch-3] and running_loss_eval_log[epoch]>running_loss_eval_log[epoch-2] and running_loss_eval_log[epoch]>running_loss_eval_log[epoch-1]):\n",
    "    #        break\n",
    "PATH = './cifar_net.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "print(running_loss_eval_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCZP-xof-Sst"
   },
   "source": [
    "### 2.5 Testing\n",
    "\n",
    "When validating the trained checkpoints (models), remember to change the model status as **Evaluation Mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGmhTdkciDt0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5Aj-QQ3GRYy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LVS22lrjqW_V",
    "outputId": "101e55cc-c2ca-481c-8def-14f7f54c6748"
   },
   "outputs": [],
   "source": [
    "# In this block you are expected to write code to load saved model and deploy it to all data in test set to \n",
    "# produce segmentation masks in png images valued 0,1,2,3, which will be used for the submission to Kaggle.\n",
    "data_path = './data/test'\n",
    "num_workers = 4\n",
    "batch_size = 1\n",
    "PATH = './cifar_net.pth'\n",
    "test_set = TestDataset(data_path)\n",
    "test_data_loader = DataLoader(dataset=test_set, num_workers=num_workers,batch_size=batch_size, shuffle=False)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "i=121 #120\n",
    "for iteration, sample in enumerate(test_data_loader):\n",
    "    with torch.no_grad():\n",
    "        img = sample\n",
    "        view_img = img\n",
    "\n",
    "\n",
    "        img = img.unsqueeze(1).repeat(1, 3, 1, 1) #Did this to get input in RGB format, I am going to try and remove this\n",
    "        mask=model(img)    \n",
    "        #img = torch.mean(img, 0)\n",
    "        mask = torch.argmax(mask.squeeze(), dim=0).detach().cpu().numpy()  \n",
    "\n",
    "\n",
    "        #print(np.unique(img1))\n",
    "        #a = torch.argmax(outputs.squeeze(), dim=0).detach().cpu().numpy()\n",
    "        #print(np.unique(a))\n",
    "        #show_image_mask(a, mask[0,...].squeeze())\n",
    "        # plt.imshow(img1[0,...].squeeze(), cmap='gray') #visualise all images in test set\n",
    "        imagename=\"cmr\"+str(i)+\"_mask.png\"\n",
    "        data_dir = './data/test/mask' #'./data/test'\n",
    "        print(i)\n",
    "        show_image_mask(view_img[0,...].squeeze(), mask, cmap='gray')\n",
    "        plt.pause(1)\n",
    "        cv2.imwrite(os.path.join(data_dir,'./',imagename),mask)\n",
    "        i =i+1\n",
    "        \n",
    "#categorical_dice()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "oY_bIP2BGSq2",
    "outputId": "d0841f2b-f676-42f5-8ac2-c0a88075d101"
   },
   "outputs": [],
   "source": [
    "data_dir = './data/test' #'./data/test'\n",
    "show_image_mask(image, mask, cmap='gray')\n",
    "plt.pause(1)\n",
    "cv2.imwrite(os.path.join('./','cmr100.png'), mask*85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsycVbIuUov3"
   },
   "source": [
    "## 3 Evaluation\n",
    "\n",
    "As we will automatically evaluate your predicted test makes on Kaggle, in this section we expect you to learn:\n",
    "* what is the Dice score used on Kaggle to measure your models performance\n",
    "* how to submit your predicted masks to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NicQyj47jsD1"
   },
   "source": [
    "### 3.1 Dice Score\n",
    "\n",
    "To evaluate the quality of the predicted masks, the Dice score is adopted. Dice score on two masks A and B is defined as the intersection ratio between the overlap area and the average area of two masks. A higher Dice suggests a better registration.\n",
    "\n",
    "$Dice (A, B)= \\frac{2|A \\cap B|}{|A| + |B|} $\n",
    "\n",
    "However, in our coursework, we have three labels in each mask, we will compute the Dice score for each label and then average the three of them as the final score. Below we have given you `categorical_dice` for free so you can test your results before submission to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzOY4GROqW_V"
   },
   "outputs": [],
   "source": [
    "def categorical_dice(mask1, mask2, label_class):\n",
    "    \"\"\"\n",
    "    Dice score of a specified class between two volumes of label masks.\n",
    "    (classes are encoded but by label class number not one-hot )\n",
    "    Note: stacks of 2D slices are considered volumes.\n",
    "\n",
    "    Args:\n",
    "        mask1: N label masks, numpy array shaped (H, W, N)\n",
    "        mask2: N label masks, numpy array shaped (H, W, N)\n",
    "        label_class: the class over which to calculate dice scores\n",
    "\n",
    "    Returns:\n",
    "        volume_dice\n",
    "    \"\"\"\n",
    "    mask1_pos = (mask1 == label_class).astype(np.float32)\n",
    "    mask2_pos = (mask2 == label_class).astype(np.float32)\n",
    "    dice = 2 * np.sum(mask1_pos * mask2_pos) / (np.sum(mask1_pos) + np.sum(mask2_pos))\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZcsrwmVjy5k"
   },
   "source": [
    "### 3.2 Submission\n",
    "\n",
    "Kaggle requires your submission to be in a specific CSV format. To help ensure your submissions are in the correct format, we have provided some helper functions to do this for you. For those interested, the png images are run-length encoded and saved in a CSV to the specifications required by our competition.\n",
    "\n",
    "It is sufficient to use this helper function. To do so, save your 80 predicted masks into a directory. ONLY the 80 predicted masks should be in this directory. Call the submission_converter function with the first argument as the directory containing your masks, and the second the directory in which you wish to save your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHDVbgu0qW_V"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    *** Credit to https://www.kaggle.com/rakhlin/fast-run-length-encoding-python ***\n",
    "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def submission_converter(mask_directory, path_to_save):\n",
    "    writer = open(os.path.join(path_to_save, \"submission.csv\"), 'w')\n",
    "    writer.write('id,encoding\\n')\n",
    "\n",
    "    files = os.listdir(mask_directory)\n",
    "\n",
    "    for file in files:\n",
    "        name = file[:-4]\n",
    "        mask = cv2.imread(os.path.join(mask_directory, file), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        mask1 = (mask == 1)\n",
    "        mask2 = (mask == 2)\n",
    "        mask3 = (mask == 3)\n",
    "\n",
    "        encoded_mask1 = rle_encoding(mask1)\n",
    "        encoded_mask1 = ' '.join(str(e) for e in encoded_mask1)\n",
    "        encoded_mask2 = rle_encoding(mask2)\n",
    "        encoded_mask2 = ' '.join(str(e) for e in encoded_mask2)\n",
    "        encoded_mask3 = rle_encoding(mask3)\n",
    "        encoded_mask3 = ' '.join(str(e) for e in encoded_mask3)\n",
    "\n",
    "        writer.write(name + '1,' + encoded_mask1 + \"\\n\")\n",
    "        writer.write(name + '2,' + encoded_mask2 + \"\\n\")\n",
    "        writer.write(name + '3,' + encoded_mask3 + \"\\n\")\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bOn_j_FqW_V"
   },
   "outputs": [],
   "source": [
    "submission_converter('./data/test/mask','./submission')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XHA7it25yTt"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2CR02PfaBrY"
   },
   "source": [
    "#Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_3zA4uMaJn9"
   },
   "source": [
    "## Introduction\n",
    "(10\\%): discuss the data sets involved, the machine learning task,\n",
    "relevant work and what you aimed to achieve.\n",
    "\n",
    "We are trying to produce masks that are semantic segmentations of magnetic resonance images using deep learning. The dataset we are working on contains 200 CMR images, 50% for training, 10% for validation and 40% for testing.  We have selected and inplemented a network architecture (U-net), trained the network and have tuned the hyperparameters to yield the best classification accuracy. Our aims was to do well in the Kaggle leaderboards.\n",
    "We started with working on finding a suitable CNN architecture. We then implemented what we found from our source and modified it to suit our problem. Then we assigned team members differeing sets of hyperparameters to experiment with whilst training the network. We then tuned our hyperparameters from our results and submitted our results to Kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwzjs5oVbjV1"
   },
   "source": [
    "## Implementation\n",
    "(35\\%): describe how you implemented your neural network and the associated performance analysis mechanisms. Explain why you chose to do it that way. Remember to cite any sources you used.\n",
    "\n",
    "As for selecting potential CNN architecures for the segmentation network, we were considering FCN2, DeepLab3, SegNet and U-net.  \n",
    "\n",
    "The convolutional neural network architecture that we have chosen to use is U-net. ***The version we have used is a mini-network of U-net as our training data is small?*** We found an existing architecture implementation at this website: https://towardsdatascience.com/train-a-lines-segmentation-model-using-pytorch-34d4adab8296. We then modified the code to suit our problem.\n",
    "\n",
    "(If we have time we can experiment with a few other CNN architectures.)\n",
    "\n",
    "For Loss functions we decided on:\n",
    "To obtain precise segmentation results, implementing an effective loss funciotn is important. Some loss functions we considered were : the cross entropy loss6, the soft dice loss7, the focal loss8, the boundary loss9, or a combination of some of them.\n",
    "For optimzers we used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CH0zOkaubjyp"
   },
   "source": [
    "## Experiment\n",
    "(40\\%): describe the experiments you carried out to optimize your\n",
    "networks generalization performance and present the results you obtained. Explain in detail how you used the training, validation and test data sets. The results should be presented in a statistically rigorous manner.\n",
    "\n",
    "For training, you are expected to show the following components: network architecture, loss fucniton, optimizer and training processing. For the training process, you should show you understanding on the number of epochs required to train you network, as well as data loading, It is also necessary to show correct training mode (model.train), zero gradient (optimizer.zero grad), backpropagation (loss.backward), optimization (optimizer.step), etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0teZmJlAbkSD"
   },
   "source": [
    "## Conclusion\n",
    "(10\\%): summarize your key findings, including which factors proved most crucial, and what was the best generalization performance you achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDVUiKiVbhJi"
   },
   "source": [
    "## References\n",
    "\n",
    "(1) Mostafa Gazar (Sep 14, 2019) Train a lines segmentation model using Pytorch. Available at https://towardsdatascience.com/train-a-lines-segmentation-model-using-pytorch-34d4adab8296\n",
    "\n",
    "(2) Olaf Ronneberger, Philipp Fischer, and Thomas Brox (May 18, 2015) U-Net: Convolutional Networks for Biomedical Image Segmentation. Available at https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tom brier cw2 instance 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
